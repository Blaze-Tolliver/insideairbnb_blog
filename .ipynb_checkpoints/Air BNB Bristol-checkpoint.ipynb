{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at Air BnB listings in Bristol, UK\n",
    "Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question One: Are there long-term and periodic trends for Airbnb usage in Bristol since the first listings in 2011?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./reviews.csv does not exist: './reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-3629dbb2b935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#importing reviews dataset, which shows date of each review for each listing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./reviews.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./reviews.csv does not exist: './reviews.csv'"
     ]
    }
   ],
   "source": [
    "#importing reviews dataset, which shows date of each review for each listing\n",
    "reviews = pd.read_csv('./reviews.csv', parse_dates=[\"date\"]) \n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews contains two columns only - the listing_id and the date of the review. There are no missing values. For the sake of this analysis, I assume the review took place in the same month as the actual stay. There is no other record of stay dates.\n",
    "\n",
    "Next I resampled reviews to calculate cummulative monthly review counts for the entire Bristol area. I considered trimming the timeseries to not include 2020 so that long-term trends pre-covid could be analyzed, but I decided to include 2020 to show how the long-term trend suddenly changes. This illustrates the uncertainty that must be accounted for in trying to extrapolate trend data into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting date as the index\n",
    "reviews.index = reviews['date']\n",
    "#resampling to monthly count of reviews for all listings combined, and limiting\n",
    "monthly_reviews = reviews.resample('M', label = 'left', loffset = '1d').count()\n",
    "monthly_reviews['reviews_per_month'] = monthly_reviews.date\n",
    "monthly_reviews = monthly_reviews.drop(['listing_id', 'date'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of monthly reviews, showing strong growth in reviews since first listing reviews in 2011. Apparent seasonality as well. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_reviews.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dramatic upward trend in reviews is clearly evident, it would be nice to apply a model that will attempt to separate out the underlying long-term trend, the seasonlity, and residual variations. I chose to use the statsmodels time series analysis seasonal decomposition filter. \n",
    "\n",
    "I used the 'additive' model option, because the results are easier for the observer to interpret. If I were going to pursue this analysis further, I would have to compare the performance and applicability of the 'additive' vs. the 'multiplicative' models. This is really just a first attempt at getting familiar with seasonality filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trend and seasonality analysis\n",
    "decomposition = sm.tsa.seasonal_decompose(monthly_reviews, model = 'additive')\n",
    "#plots of trend, seasonality, and residuals for the additive seasonal decomposition model\n",
    "fig = decomposition.plot()\n",
    "plt.rcParams['figure.figsize'] = [12.0, 12.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter identified some seasonality, but in reality the model only fits decently in 2016. In the years before 2016, the residuals are canceling out the seasonality. In the years after 2016, the residuals are boosting the seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting trend line and actual reviews per month\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(True)\n",
    "plt.plot(monthly_reviews.index, monthly_reviews['reviews_per_month'], c='blue', label = 'Data')\n",
    "plt.plot(decomposition.trend.index, decomposition.trend, c='red', label = 'Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Reviews per Month')\n",
    "plt.title('Growth in Bristol Airbnb Usage')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question One Conclusion\n",
    "Since 2011, there has been a consistent uptrend in listing reviews. However, Coronavirus lock-down means that the uptrend is now broken. It will be interesting to see how long it takes for the uptrend to continue after restrictions are eased. \n",
    "\n",
    "There is a seasonal component to the variation in listing reviews. Due to the rapid growth in listing reviews, it is difficult to isolate the seasonal contribution to variation without applying more advanced modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions Two & Three: What does it take to have a frequently visited listing that remains popular long-term? Can the written comment sentiment be used to approximate the average numeric rating for listings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here's a more detailed reviews dataset that has written comments from the reviews\n",
    "reviews_detail = pd.read_csv('./reviews_detail.csv', parse_dates=[\"date\"]) \n",
    "reviews_detail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the TextBlog sentiment analysis tool to see if I could distinguish well reviewed listings from poorly reviewed listings. The reviews_detail dataset doesn't contain the numeric review scores. I will have to relate the aggregated review sentiment to the aggregated review score from the list_detail dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating sentiment polarity for each review comment, using TextBlob\n",
    "sentiment = []\n",
    "for row in reviews_detail.comments:\n",
    "    text = str(row)\n",
    "    comment = TextBlob(text)\n",
    "    polarity = comment.sentiment.polarity\n",
    "    sentiment.append(polarity)\n",
    "\n",
    "#append the sentiment list to reviews_detail df\n",
    "reviews_detail['sentiment'] = sentiment \n",
    "\n",
    "#aggregate mean review sentiment polarity for each listing_id, and drop listings with < 1 comments\n",
    "listing_mean_sent = pd.concat([reviews_detail.groupby('listing_id')['sentiment'].median(), reviews_detail.groupby('listing_id')['comments'].count()], axis = 1)\n",
    "listing_mean_sent = listing_mean_sent[listing_mean_sent.comments >= 1]\n",
    "listing_mean_sent.comments.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have the mean sentiment polarity score for each listing that had at least 1 written review comment. Polarity can range from -1 to 1, where -1 is entirely negative, 0 is neutral, and 1 is entirely positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the mean listing sentiment polarity to the listing details df\n",
    "#the list_detail dateset contains the average review scores for each listing\n",
    "list_detail = pd.read_csv('./listings_detail.csv', parse_dates=[\"last_review\", 'first_review'])\n",
    "list_detail.index =   list_detail['id']\n",
    "list_detail = list_detail.merge(listing_mean_sent, how = 'left', left_index = True, right_index = True)\n",
    "print(*list_detail.columns, sep = ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I wanted to know how long each listing was active, so I calculated the timedelta in years between the first and last reviews. This will be used as the hue in the scatterplot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the listing age by difference between first review date and last review date\n",
    "current = pd.to_datetime('2020-04-01')\n",
    "list_detail.first_review\n",
    "list_detail['listing_age'] = list_detail.last_review.dt.year - list_detail.first_review.dt.year\n",
    "list_detail.listing_age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot below is intended to help visualize what it takes to have a popular listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scatter plot of listing score vs. number of comments\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x = list_detail.review_scores_rating, y = list_detail.comments, hue = list_detail.listing_age, size = list_detail.listing_age, sizes = (10,400), legend = 'brief', alpha = 0.5)\n",
    "plt.xlabel('Listing Review Score')\n",
    "plt.ylabel('Number of Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the relationship between mean listing review score and number of reviews, it's pretty clear that listings that get poor reviews don't get many visitors, and generally don't last for many years. It also appears to take a few years to accummulate hundreds of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_detail.review_scores_rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Conclusion\n",
    "Looking at the relationship between mean listing review score and number of reviews, it's pretty clear that listings that get poor reviews don't get many visitors. It's also clear that poorly reviewed listings generally don't last for many years. \n",
    "\n",
    "In fact, in order to even register in the 25th percentile of mean listing scores, your listing will need to garner a 92%! To be average (median), you'll need a 97%.\n",
    "\n",
    "People seem to be pretty risk averse when it comes to choosing an Airbnb. Therefore, it would behoove anyone considering listing a place to be on their A-game before opening up to visitors.\n",
    "\n",
    "On the other hand, staying at an Airbnb is a little more personal of an experience than buying something online. Therefore, it feels a bit more confrontational to leave a poor Airbnb review. I think that is partially why the reviews scores tend to be so high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text sentiment analysis\n",
    "For the text sentiment analysis, I chose to consider a bad listing to have a review score rating below 70% in order to give the sentiment analysis the absolute worst listings to compare with the absolute best listings, which I defined as a review score rating >= 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comparing sentiment polarity to review scores\n",
    "sentiment_accuracy = list_detail[['sentiment','review_scores_rating']].dropna(axis = 0, how = 'any')\n",
    "sentiment_accuracy['score_good'] = np.where(sentiment_accuracy['review_scores_rating'] >= 70, 'Ok', 'bad')\n",
    "sentiment_accuracy['score_good'].where(sentiment_accuracy['review_scores_rating'] < 97.0, other = 'great', inplace = True)\n",
    "sentiment_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scores are categorized: less than 70% is bad, 70 to <97% is OK, and >= 97% is great "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_accuracy = sentiment_accuracy.drop(['review_scores_rating'], axis = 1)\n",
    "sentiment_accuracy_pivot = sentiment_accuracy.pivot(columns = 'score_good', values = 'sentiment')\n",
    "sns.boxplot(x = sentiment_accuracy.score_good, y = sentiment_accuracy.sentiment, orient = 'v')\n",
    "plt.xlabel('Listing Review Score')\n",
    "plt.ylabel('Listing Median Sentiment')\n",
    "#plt.savefig(fname = 'sentiment box.jpg', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Conclusion\n",
    "The interquartile range (IQR of listing median sentiment for bad listings does not overlap with the IQR of OK and great listings. This is just a first brush at using text sentiment analysis. As it is, I could see a use for it in getting an overall picture of review scores. However, to be useful in distinguishing which score category a particular listing falls in, more work would be needed. Of course there is no guarantee that people's written comments convey the sentiment of the score they leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Question 4: What are the price distributions in each neighborhood for short-term listings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter out listings with no 'last review'\n",
    "list_detail = pd.read_csv('./listings_detail.csv', parse_dates=[\"last_review\", 'first_review'])\n",
    "list_detail_reviewed = list_detail.dropna(axis = 0, subset = ['last_review'])\n",
    "list_detail_reviewed.last_review.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for listings with recent reviews. I'm interested in current pricing.\n",
    "list_detail_current = list_detail_reviewed[list_detail_reviewed['last_review'] > '2018-12-31' ]\n",
    "list_detail_current.last_review.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value counts indicate that filtering out listings with min nights > 3 will not reduce our observations very much\n",
    "#I'm interested in short-term rental pricing\n",
    "longstay = list_detail_current.minimum_nights_avg_ntm > 3.0\n",
    "longstay.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out listings requiring > 3 night minimum stay\n",
    "list_detail_current = list_detail_current[list_detail_current['minimum_nights_avg_ntm'] <= 3.0 ]\n",
    "list_detail_current.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting neighborhoods that have at least 10 listings, and preparing to barplot\n",
    "#I want representative pricing samples of neighborhoods, so I had to pick a minimum listing threshold\n",
    "neighborhood = pd.concat([list_detail_current['neighbourhood_cleansed'], list_detail_current['price'].str.replace('$', '').str.replace(',', '').astype(float)], axis = 1)\n",
    "neighborhood.index = neighborhood['neighbourhood_cleansed']\n",
    "neighborhood['min_listings'] = neighborhood['neighbourhood_cleansed'].groupby('neighbourhood_cleansed').count() > 9\n",
    "neighborhood = neighborhood[neighborhood.min_listings]\n",
    "neighborhood = neighborhood.reset_index(drop = True)\n",
    "neighborhood = neighborhood.drop(['min_listings'], axis = 1)\n",
    "neighborhood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but first I want to tabulate median prices by neighborhood inorder to sort my barplot by median price\n",
    "neighborhood_median_price = neighborhood.groupby('neighbourhood_cleansed')['price'].median()\n",
    "neighborhood_median_price = neighborhood_median_price.sort_values(ascending = False)\n",
    "neighborhood_median_price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot of neighborhood prices, ordered by neighborhood median price\n",
    "neighborhood_pivot = neighborhood.pivot(columns = 'neighbourhood_cleansed', values = 'price')\n",
    "neighborhood_pivot[neighborhood_median_price.index].plot.box(vert=False, figsize=(15, 15), sym = '+')\n",
    "plt.xlabel('Price in GBP')\n",
    "plt.ylabel('Neighborhood')\n",
    "plt.title('Price Distribution by Neighborhood')\n",
    "#plt.savefig(fname = 'neighborhood_price_box.jpg', bbox_inches = 'tight', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the price outliers are dominating the figure. Some of the prices may be errors, like prices that are meant for weekly or monthly rentals. Some places may be event or wedding venues. I decided to filter out the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighborhood_pivot[neighborhood_median_price.index].plot.box(vert=False, figsize=(15, 15), sym = '')\n",
    "plt.xlabel('Price in GBP')\n",
    "plt.ylabel('Neighborhood')\n",
    "plt.title('Price Distribution by Neighborhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Conclusion\n",
    "This information is useful for local government planning purposes, or potentially useful for an individual looking to list with Airbnb. Clearly there are differences between neighborhood pricing, but it isn't clear how to price your listing until you compare with similar listings. However, if there aren't any similar listings in your neighborhood, but are in other neighborhoods, you could use this figure to convert the listing price to your neighborhood's relative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
